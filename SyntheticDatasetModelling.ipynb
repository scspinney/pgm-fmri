{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.11"
    },
    "colab": {
      "name": "SyntheticDatasetModelling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8WvwDm7yS_c",
        "outputId": "d8e7cf6b-6697-45e3-c0c0-0f6facb934f6"
      },
      "source": [
        "!pip install -q git+git://github.com/deepmind/optax.git\n",
        "!pip install -q git+https://github.com/deepmind/dm-haiku"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |██████▉                         | 10kB 17.3MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 20kB 21.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 30kB 10.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 40kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 2.6MB/s \n",
            "\u001b[?25h  Building wheel for optax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for dm-haiku (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXb_vIxux9Cr"
      },
      "source": [
        "from typing import Any, Generator, Mapping, Tuple\n",
        "\n",
        "from absl import app\n",
        "import haiku as hk\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import optax\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from optax._src import transform\n",
        "from jax import jit, grad, vmap\n",
        "from jax.tree_util import tree_structure\n",
        "from numpy.random import default_rng\n",
        "\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "np.set_printoptions(precision=2,suppress=True)\n",
        "np.random.seed(0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-jwVKt_x9C7"
      },
      "source": [
        "# for every environment, pick at random a strongest predictor (does not have to be unique) + random small coefficients for all others\n",
        "# EXCEPT for a subset of predictors which are consistent across all environments\n",
        "\n",
        "def generate_synthetic_dataset(E,ntrain,glob_sigma,sig_sigma,K,rand_strong_coefs,inv_weak_coefs,true_pred_indices,n_informative,n_redundant,use_sklearn):\n",
        "    # more info for sklearn dataset generation: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html#sklearn.datasets.make_classification\n",
        "    dist = torch.distributions.Uniform(-1, 1)\n",
        "    if use_sklearn:\n",
        "      X,y = make_classification(n_samples=E, \n",
        "                          n_features=K, \n",
        "                          n_informative=n_informative,\n",
        "                          n_redundant=n_redundant, \n",
        "                          n_repeated=0, \n",
        "                          n_classes=2, \n",
        "                          n_clusters_per_class=2, \n",
        "                          weights=None, \n",
        "                          flip_y=sig_sigma, \n",
        "                          class_sep=glob_sigma, \n",
        "                          hypercube=True, \n",
        "                          shift=0.0, \n",
        "                          scale=1.0, \n",
        "                          shuffle=False, \n",
        "                          random_state=np.random.seed(0))\n",
        "    else:\n",
        "      rng = default_rng()\n",
        "      #np.random.seed(100)\n",
        "      X = np.random.normal(0, glob_sigma, (E, K))\n",
        "      #y = np.random.randint(2,size=E)\n",
        "      y = rng.choice(2, size=E, replace=True)\n",
        "\n",
        "      for e in range(E):\n",
        "        # new seed\n",
        "        #np.random.seed(e)\n",
        "        # pick a subset of best predictors at random \n",
        "        #rand_best_indices = np.random.randint(K, size=3)\n",
        "        rand_best_indices = rng.choice(K, size=1, replace=False)\n",
        "\n",
        "        # check to make sure these randoms aren't out true pred\n",
        "        condition = True\n",
        "        while condition:\n",
        "          for i,rbp in enumerate(rand_best_indices):\n",
        "            if rbp in true_pred_indices:\n",
        "              #rand_best_indices[i] = np.random.randint(K, size=1)\n",
        "              rand_best_indices[i] = rng.choice(K, size=1, replace=False)\n",
        "              condition = True\n",
        "              break\n",
        "            else:\n",
        "              condition = False\n",
        "        #print(true_pred_indices,rand_best_indices)\n",
        "        # set the coefficients at the right row \n",
        "        # that separates each class\n",
        "        if y[e] == 1:\n",
        "          X[e,true_pred_indices] = inv_weak_coefs # +  np.random.normal(0,sig_sigma)\n",
        "          X[e,rand_best_indices] = rand_strong_coefs + dist.sample(X[e,rand_best_indices].shape)  #np.random.normal(0,sig_sigma)\n",
        "\n",
        "        elif y[e] == 0:\n",
        "          X[e,true_pred_indices] = 0 #+  np.random.normal(0,sig_sigma) #-inv_weak_coefs \n",
        "          X[e,rand_best_indices] = 0 + dist.sample(X[e,rand_best_indices].shape)\n",
        "\n",
        "\n",
        "    #Z = np.c_[ X, y ]   \n",
        "    #print(np.corrcoef(Z.T))\n",
        "    print(X,y)\n",
        "    return y,X\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeCJncByyrU1"
      },
      "source": [
        "class ANDMaskState(optax.OptState):\n",
        "  \"\"\"Stateless.\"\"\" # Following optax code style\n",
        "\n",
        "def and_mask(agreement_threshold: float) -> optax.GradientTransformation:\n",
        "  def init_fn(_):\n",
        "    # Required by optax\n",
        "    return ANDMaskState()\n",
        "\n",
        "  def update_fn(updates, opt_state, params=None):\n",
        "\n",
        "    def and_mask(update):\n",
        "      # Compute the masked gradients for a single parameter tensor\n",
        "      mask = jnp.abs(jnp.mean(jnp.sign(update), 0)) >= agreement_threshold\n",
        "      mask = mask.astype(jnp.float32)\n",
        "      avg_update = jnp.mean(update, 0)\n",
        "      mask_t = mask.sum() / mask.size\n",
        "      update = mask * avg_update * (1. / (1e-10 + mask_t))\n",
        "      return update\n",
        "\n",
        "    del params # Following optax code style\n",
        "    \n",
        "    # Compute the masked gradients over all parameters\n",
        "\n",
        "    # jax.tree_map maps a function (lambda function in this case) over a pytree to produce a new pytree.\n",
        "    updates = jax.tree_map(lambda x: and_mask(x), updates)\n",
        "    return updates, opt_state\n",
        "\n",
        "  return transform.GradientTransformation(init_fn, update_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co_h9GFpx9C8"
      },
      "source": [
        "def sparse_logistic_regression_synthetic_data(train, test, adam_lr=1e-3, agreement_threshold=0.9,\n",
        "                               use_ilc=False, l1_coef=1e-3, l2_coef=1e-3,\n",
        "                               epochs=100, Verbose=False, n_classes=2, normalizer=255.):\n",
        "  \n",
        "    OptState = Any\n",
        "    Batch = Mapping[str, np.ndarray]\n",
        "\n",
        "    def evaluate(params, validation_data, validation_labels):\n",
        "    \n",
        "        logits = net.apply(params, validation_data)\n",
        "        preds = jax.nn.log_softmax(logits)\n",
        "        pred_labels = np.argmax(preds, axis=1)\n",
        "\n",
        "        acc = accuracy(params, validation_data, validation_labels)\n",
        "        # As mentioned before, data is unbalanced, hence, the accuracy itself is not \n",
        "        # enough for evaluating the performance of the model.\n",
        "        # print(outputs,local_labels.cpu().detach().numpy())\n",
        "        cm = confusion_matrix(pred_labels.transpose(), validation_labels.transpose())\n",
        "        #sns.set_theme()\n",
        "        #plt.figure()\n",
        "        #ax = sns.heatmap(cm)\n",
        "        print('\\nConfusion Matrix: ', cm)\n",
        "        precision,recall,fscore,_ = precision_recall_fscore_support(validation_labels, pred_labels)\n",
        "        print('\\nAccuracy: ', acc,'\\nPrecision: ',precision,'\\nRecall: ', recall,'\\nF-score: ', fscore)\n",
        "\n",
        "\n",
        "    def net_fn(batch) -> jnp.ndarray:\n",
        "    \n",
        "        x = jnp.array(batch, jnp.float32) / normalizer\n",
        "        mlp = hk.Sequential([\n",
        "            hk.Flatten(),\n",
        "            hk.Linear(n_classes),\n",
        "        ])\n",
        "        return mlp(x)\n",
        "\n",
        "\n",
        "    # Make the network and optimiser.\n",
        "    net = hk.without_apply_rng(hk.transform(net_fn))\n",
        "\n",
        "    \n",
        "       \n",
        "    # Training loss (cross-entropy).\n",
        "    def loss(params: hk.Params, batch, label) -> jnp.ndarray:\n",
        "        \"\"\"Compute the loss of the network, including L2.\"\"\"\n",
        "        logits = net.apply(params, batch)\n",
        "        labels = jax.nn.one_hot(label, n_classes)\n",
        "\n",
        "        # # L1 Loss\n",
        "        # sum_in_layer = lambda p: jnp.sum(jnp.abs(p))\n",
        "        # sum_p_layers = [sum_in_layer(p) for p in jax.tree_leaves(params)]\n",
        "        # l1_loss = sum(sum_p_layers)\n",
        "\n",
        "        # # L2 Loss\n",
        "        # l2_loss = 0.5 * sum(jnp.sum(jnp.square(p)) for p in jax.tree_leaves(params))\n",
        "\n",
        "        # Cross Entropy Loss\n",
        "        softmax_xent = -jnp.sum(labels * jax.nn.log_softmax(logits))\n",
        "        softmax_xent /= labels.shape[0]\n",
        "        return softmax_xent # + 1e-4 * l2_loss + 1e-5 * l1_loss\n",
        "\n",
        "\n",
        "    \n",
        "        \n",
        "    # Regularization loss (L1,L2).\n",
        "    def regularization_loss(params: hk.Params) -> jnp.ndarray:\n",
        "        \"\"\"Compute the regularization loss of the network, applied after ILC.\"\"\"\n",
        "\n",
        "        # L1 Loss\n",
        "        sum_in_layer = lambda p: jnp.sum(jnp.abs(p))\n",
        "        sum_p_layers = [sum_in_layer(p) for p in jax.tree_leaves(params)]\n",
        "        l1_loss = sum(sum_p_layers)\n",
        "\n",
        "        # L2 Loss\n",
        "        l2_loss = 0.5 * sum(jnp.sum(jnp.square(p)) for p in jax.tree_leaves(params))\n",
        "\n",
        "        return l2_coef * l2_loss + l1_coef * l1_loss\n",
        "\n",
        "\n",
        "    @jax.jit\n",
        "    def gradient_per_sample(params, batch, label):\n",
        "        batch, label = jnp.expand_dims(batch,1), jnp.expand_dims(label,1)\n",
        "        return vmap(grad(loss), in_axes=(None, 0, 0))(params, batch, label)\n",
        "\n",
        "    \n",
        "    gradient = jax.jit(grad(loss))\n",
        "    gradient_reg = jax.jit(grad(regularization_loss))\n",
        "\n",
        "    # Evaluation metric (classification accuracy).\n",
        "    @jax.jit\n",
        "    def accuracy(params: hk.Params, batch, label) -> jnp.ndarray:\n",
        "        predictions = net.apply(params, batch)\n",
        "        return jnp.mean(jnp.argmax(predictions, axis=-1) == label)\n",
        "\n",
        "\n",
        "    \n",
        "    @jax.jit\n",
        "    def update(\n",
        "        params: hk.Params,\n",
        "        opt_state: OptState,\n",
        "        batch, label\n",
        "        ) -> Tuple[hk.Params, OptState]:\n",
        "        \"\"\"Learning rule (stochastic gradient descent).\"\"\"\n",
        "        # grads = jax.grad(loss)(params, batch, label)\n",
        "        grads_masked = (gradient_per_sample if use_ilc else gradient)(params, batch, label) # (gradient_per_sample)(params, batch, label)\n",
        "        sum_grad_masked_regularized = jax.tree_multimap(lambda x,y:x+y,grads_masked,gradient_reg(params))\n",
        "        grads = sum_grad_masked_regularized\n",
        "        updates, opt_state = opt.update(grads, opt_state)\n",
        "        new_params = optax.apply_updates(params, updates)\n",
        "        return new_params, opt_state\n",
        "\n",
        "\n",
        "    # We maintain avg_params, the exponential moving average of the \"live\" params.\n",
        "    # avg_params is used only for evaluation.\n",
        "    # For more, see: https://doi.org/10.1137/0330046\n",
        "    @jax.jit\n",
        "    def ema_update(\n",
        "        avg_params: hk.Params,\n",
        "        new_params: hk.Params,\n",
        "        epsilon: float = 0.001,\n",
        "    ) -> hk.Params:\n",
        "        return jax.tree_multimap(lambda p1, p2: (1 - epsilon) * p1 + epsilon * p2,\n",
        "                                avg_params, new_params)\n",
        "        \n",
        "    \n",
        "    \n",
        "    if use_ilc:\n",
        "\n",
        "        use_ilc = False\n",
        "\n",
        "        opt = optax.chain(optax.adam(adam_lr)\n",
        "            # ,optax.adam(adam_lr)\n",
        "            # ,optax.scale_by_adam()\n",
        "            )\n",
        "        # Initialize network and optimiser; note we draw an input to get shapes.\n",
        "        params = avg_params = net.init(jax.random.PRNGKey(42), next(train)[0])\n",
        "        opt_state = opt.init(params)\n",
        "\n",
        "        # Train/eval loop. WITHOUT ILC\n",
        "\n",
        "        print(\"Begin training with ILC\")\n",
        "        for step in range(np.int(.4*epochs)):\n",
        "            if step % np.int(epochs/10) == 0:\n",
        "                # Periodically evaluate classification accuracy on train & test sets.\n",
        "                Batch = next(train)\n",
        "                train_accuracy = accuracy(avg_params, Batch[0], Batch[1])\n",
        "                train_accuracy = jax.device_get(train_accuracy)\n",
        "                Batch = next(test)\n",
        "                test_accuracy = accuracy(avg_params, Batch[0], Batch[1])\n",
        "                test_accuracy = jax.device_get(test_accuracy)\n",
        "                if Verbose:\n",
        "                    print(f\"[Step {step}] Train accuracy/Test accuracy: \"\n",
        "                            f\"{train_accuracy:.3f} / {test_accuracy:.3f}.\")\n",
        "\n",
        "            # Do SGD on a batch of training examples.\n",
        "            Batch = next(train)\n",
        "            params, opt_state = update(params, opt_state, Batch[0], Batch[1])\n",
        "            avg_params = ema_update(avg_params, params)\n",
        "\n",
        "\n",
        "        \n",
        "        use_ilc=True\n",
        "        opt = optax.chain(and_mask(agreement_threshold)\n",
        "            ,optax.adam(adam_lr)\n",
        "            # ,optax.adam(adam_lr)\n",
        "            # ,optax.scale_by_adam()\n",
        "            )\n",
        "        \n",
        "        # Initialize network and optimiser; note we draw an input to get shapes.\n",
        "        opt_state = opt.init(params)\n",
        "\n",
        "        # Train/eval loop. WITH ILC\n",
        "        for step in range(np.int(.6*epochs)):\n",
        "            if step % np.int(epochs/10) == 0:\n",
        "                # Periodically evaluate classification accuracy on train & test sets.\n",
        "                Batch = next(train)\n",
        "                train_accuracy = accuracy(avg_params, Batch[0], Batch[1])\n",
        "                train_accuracy = jax.device_get(train_accuracy)\n",
        "                Batch = next(test)\n",
        "                test_accuracy = accuracy(avg_params, Batch[0], Batch[1])\n",
        "                test_accuracy = jax.device_get(test_accuracy)\n",
        "                if Verbose:\n",
        "                    print(f\"[Step {step}] Train accuracy/Test accuracy: \"\n",
        "                            f\"{train_accuracy:.3f} / {test_accuracy:.3f}.\")\n",
        "\n",
        "            # Do SGD on a batch of training examples.\n",
        "            Batch = next(train)\n",
        "            params, opt_state = update(params, opt_state, Batch[0], Batch[1])\n",
        "            avg_params = ema_update(avg_params, params)\n",
        "\n",
        "        print(\"Evaluating with ILC:\")\n",
        "        Batch = next(test)\n",
        "        test_data, test_labels = Batch[0], Batch[1]\n",
        "        evaluate(avg_params, test_data, test_labels)\n",
        "    \n",
        "    else:\n",
        "      opt = optax.chain(optax.adam(adam_lr)\n",
        "          # ,optax.adam(adam_lr)\n",
        "          # ,optax.scale_by_adam()\n",
        "          )\n",
        "      # Initialize network and optimiser; note we draw an input to get shapes.\n",
        "      params = avg_params = net.init(jax.random.PRNGKey(42), next(train)[0])\n",
        "      opt_state = opt.init(params)\n",
        "      #\n",
        "      use_ilc=False\n",
        "      # Train/eval loop. \n",
        "      print(\"Begin training without ILC\")\n",
        "      for step in range(np.int(epochs)):\n",
        "          if step % np.int(epochs/10) == 0:\n",
        "              # Periodically evaluate classification accuracy on train & test sets.\n",
        "              Batch = next(train)\n",
        "              train_accuracy = accuracy(avg_params, Batch[0], Batch[1])\n",
        "              train_accuracy = jax.device_get(train_accuracy)\n",
        "              Batch = next(test)\n",
        "              test_accuracy = accuracy(avg_params, Batch[0], Batch[1])\n",
        "              test_accuracy = jax.device_get(test_accuracy)\n",
        "              if Verbose:\n",
        "                  print(f\"[Step {step}] Train accuracy/Test accuracy: \"\n",
        "                          f\"{train_accuracy:.3f} / {test_accuracy:.3f}.\")\n",
        "\n",
        "          # Do SGD on a batch of training examples.\n",
        "          Batch = next(train)\n",
        "          params, opt_state = update(params, opt_state, Batch[0], Batch[1])\n",
        "          avg_params = ema_update(avg_params, params)\n",
        "      \n",
        "      print(\"Evaluating without ILC:\")\n",
        "      Batch = next(test)\n",
        "      test_data, test_labels = Batch[0], Batch[1]\n",
        "      evaluate(avg_params, test_data, test_labels)\n",
        "\n",
        "\n",
        "\n",
        "    return params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxm8xq8hqfck"
      },
      "source": [
        "OptState = Any\r\n",
        "Batch = Mapping[str, np.ndarray]\r\n",
        "\r\n",
        "def sparse_logistic_regression(train, adam_lr=1e-3, agreement_threshold=0.0,\r\n",
        "                               use_ilc=False, l1_coef=1e-5, l2_coef=1e-4,\r\n",
        "                               epochs=10001, Verbose=False, n_classes=10, normalizer=255.):\r\n",
        "\r\n",
        "\r\n",
        "    return params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hm9UUFbBx9C4"
      },
      "source": [
        "rng = default_rng()\n",
        "E = 2000 # number of environments\n",
        "train_batch_size = np.float32(.5*E)\n",
        "sigma = [1] # amount of noise \n",
        "signal_noise = [2]\n",
        "K = 10 # number of predictors (at least 40)\n",
        "n_informative = 5\n",
        "n_redundant = 0\n",
        "nepochs = 3000\n",
        "weights=[0.7,0.3]\n",
        "rand_strong_coefs = 3.\n",
        "inv_weak_coefs = 0.3\n",
        "use_sklearn = False\n",
        "# true invariant predictor indices \n",
        "#true_pred_indices = np.random.randint(K, size=n_informative)\n",
        "true_pred_indices = rng.choice(K, size=n_informative, replace=False)\n",
        "true_pred_indices\n",
        "agreement = 0.4\n",
        "l1_sgd = 1e-3\n",
        "l2_sgd = 1e-3\n",
        "l1_ilc = 1e-3\n",
        "l2_ilc = 1e-3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JUMVsA1x9C_",
        "outputId": "577c7edd-db8d-41b9-c2e6-fb85b0abc627"
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "\n",
        "for global_noise in sigma:\n",
        "    for sig_noise in signal_noise:\n",
        "        print(f\"Global noise: {global_noise}, Signal noise: {sig_noise}\")\n",
        "        y,x = generate_synthetic_dataset(E,train_batch_size,global_noise,sig_noise,K,rand_strong_coefs,inv_weak_coefs,true_pred_indices,n_informative,n_redundant,use_sklearn)\n",
        "        \n",
        "        test_batch_size = E-train_batch_size\n",
        "        # ds = tfds.load(\"mnist:3.*.*\", split=splits[m]).cache().repeat()\n",
        "        # print(ds)\n",
        "        all_dataset = tf.data.Dataset.from_tensor_slices((x, y)).repeat()\n",
        "        train_dataset = all_dataset.take(train_batch_size).repeat()\n",
        "        test_dataset = all_dataset.skip(train_batch_size).repeat()\n",
        "        # print(train_dataset)\n",
        "        # print(test_dataset)\n",
        "        train_dataset = train_dataset.shuffle(10.0 * train_batch_size, seed=0)\n",
        "        train_dataset = train_dataset.batch(train_batch_size)\n",
        "        train_dataset = iter(tfds.as_numpy(train_dataset))\n",
        "\n",
        "        test_dataset = test_dataset.shuffle(10.0 * test_batch_size, seed=0)\n",
        "        test_dataset = test_dataset.batch(test_batch_size)\n",
        "        test_dataset = iter(tfds.as_numpy(test_dataset))\n",
        "        \n",
        "        print(\"***Using Elastic Net***\")\n",
        "        # sgd\n",
        "        params = sparse_logistic_regression_synthetic_data(train_dataset, test_dataset, adam_lr=1e-3, agreement_threshold=agreement,\n",
        "                                       use_ilc=False, l1_coef=l1_sgd, l2_coef=l2_sgd,\n",
        "                                       epochs=nepochs, Verbose=True, n_classes=2, normalizer=1.)\n",
        "        # print(params)\n",
        "        # sgd-ilc\n",
        "        params = sparse_logistic_regression_synthetic_data(train_dataset, test_dataset, adam_lr=1e-3, agreement_threshold=agreement,\n",
        "                                use_ilc=True, l1_coef=l1_ilc, l2_coef=l2_ilc,\n",
        "                                epochs=nepochs, Verbose=True, n_classes=2, normalizer=1.)\n",
        "        # print(params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Global noise: 1, Signal noise: 2\n",
            "[[ 0.    0.98  0.   ...  0.32  0.68  0.  ]\n",
            " [ 0.    0.36  0.   ... -0.49  0.88  0.  ]\n",
            " [ 0.3  -1.5   0.3  ... -1.   -0.57  0.3 ]\n",
            " ...\n",
            " [ 0.3  -0.58  0.3  ... -1.55 -0.7   0.3 ]\n",
            " [ 0.   -0.53  0.   ... -0.57 -0.75  0.  ]\n",
            " [ 0.   -0.36  0.   ... -0.86 -0.83  0.  ]] [0 0 1 ... 1 0 0]\n",
            "***Using Elastic Net***\n",
            "Begin training without ILC\n",
            "[Step 0] Train accuracy/Test accuracy: 0.529 / 0.534.\n",
            "[Step 300] Train accuracy/Test accuracy: 0.616 / 0.593.\n",
            "[Step 600] Train accuracy/Test accuracy: 0.689 / 0.735.\n",
            "[Step 900] Train accuracy/Test accuracy: 0.822 / 0.809.\n",
            "[Step 1200] Train accuracy/Test accuracy: 0.892 / 0.903.\n",
            "[Step 1500] Train accuracy/Test accuracy: 0.942 / 0.944.\n",
            "[Step 1800] Train accuracy/Test accuracy: 0.983 / 0.976.\n",
            "[Step 2100] Train accuracy/Test accuracy: 0.989 / 0.979.\n",
            "[Step 2400] Train accuracy/Test accuracy: 0.999 / 0.990.\n",
            "[Step 2700] Train accuracy/Test accuracy: 1.000 / 0.996.\n",
            "Evaluating without ILC:\n",
            "\n",
            "Confusion Matrix:  [[520   1]\n",
            " [  0 479]]\n",
            "\n",
            "Accuracy:  0.9990001 \n",
            "Precision:  [1. 1.] \n",
            "Recall:  [1. 1.] \n",
            "F-score:  [1. 1.]\n",
            "Begin training with ILC\n",
            "[Step 0] Train accuracy/Test accuracy: 0.534 / 0.547.\n",
            "[Step 300] Train accuracy/Test accuracy: 0.608 / 0.604.\n",
            "[Step 600] Train accuracy/Test accuracy: 0.736 / 0.719.\n",
            "[Step 900] Train accuracy/Test accuracy: 0.823 / 0.838.\n",
            "[Step 0] Train accuracy/Test accuracy: 0.891 / 0.892.\n",
            "[Step 300] Train accuracy/Test accuracy: 0.941 / 0.935.\n",
            "[Step 600] Train accuracy/Test accuracy: 0.965 / 0.969.\n",
            "[Step 900] Train accuracy/Test accuracy: 0.973 / 0.966.\n",
            "[Step 1200] Train accuracy/Test accuracy: 0.975 / 0.964.\n",
            "[Step 1500] Train accuracy/Test accuracy: 0.991 / 0.977.\n",
            "Evaluating with ILC:\n",
            "\n",
            "Confusion Matrix:  [[518   4]\n",
            " [ 13 465]]\n",
            "\n",
            "Accuracy:  0.98300004 \n",
            "Precision:  [0.99 0.97] \n",
            "Recall:  [0.98 0.99] \n",
            "F-score:  [0.98 0.98]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQ5GBWwp2TRF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}