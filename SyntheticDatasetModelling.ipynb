{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.11"
    },
    "colab": {
      "name": "SyntheticDatasetModelling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8WvwDm7yS_c",
        "outputId": "2e521488-c2de-4ad5-c90c-1b8362592b8e"
      },
      "source": [
        "!pip install -q git+git://github.com/deepmind/optax.git\n",
        "!pip install -q git+https://github.com/deepmind/dm-haiku"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |██████▉                         | 10kB 25.6MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 20kB 31.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 30kB 22.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 40kB 16.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 6.3MB/s \n",
            "\u001b[?25h  Building wheel for optax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for dm-haiku (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXb_vIxux9Cr"
      },
      "source": [
        "from typing import Any, Generator, Mapping, Tuple\n",
        "\n",
        "from absl import app\n",
        "import haiku as hk\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import optax\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from optax._src import transform\n",
        "from jax import jit, grad, vmap\n",
        "from jax.tree_util import tree_structure\n",
        "from numpy.random import default_rng\n",
        "\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "np.set_printoptions(precision=2,suppress=True)\n",
        "np.random.seed(0)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-jwVKt_x9C7"
      },
      "source": [
        "# for every environment, pick at random a strongest predictor (does not have to be unique) + random small coefficients for all others\n",
        "# EXCEPT for a subset of predictors which are consistent across all environments\n",
        "\n",
        "def generate_synthetic_dataset(E,ntrain,glob_sigma,sig_sigma,K,rand_strong_coefs,inv_weak_coefs,true_pred_indices,n_informative,n_redundant,use_sklearn):\n",
        "    # more info for sklearn dataset generation: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html#sklearn.datasets.make_classification\n",
        "    dist = torch.distributions.Uniform(-1, 1)\n",
        "    if use_sklearn:\n",
        "      X,y = make_classification(n_samples=E, \n",
        "                          n_features=K, \n",
        "                          n_informative=n_informative,\n",
        "                          n_redundant=n_redundant, \n",
        "                          n_repeated=0, \n",
        "                          n_classes=2, \n",
        "                          n_clusters_per_class=2, \n",
        "                          weights=None, \n",
        "                          flip_y=sig_sigma, \n",
        "                          class_sep=glob_sigma, \n",
        "                          hypercube=True, \n",
        "                          shift=0.0, \n",
        "                          scale=1.0, \n",
        "                          shuffle=False, \n",
        "                          random_state=np.random.seed(0))\n",
        "    else:\n",
        "      rng = default_rng()\n",
        "      #np.random.seed(100)\n",
        "      X = np.random.normal(0, glob_sigma, (E, K))\n",
        "      #y = np.random.randint(2,size=E)\n",
        "      y = rng.choice(2, size=E, replace=True)\n",
        "\n",
        "      for e in range(E):\n",
        "        # new seed\n",
        "        #np.random.seed(e)\n",
        "        # pick a subset of best predictors at random \n",
        "        #rand_best_indices = np.random.randint(K, size=3)\n",
        "        rand_best_indices = rng.choice(K, size=1, replace=False)\n",
        "\n",
        "        # check to make sure these randoms aren't out true pred\n",
        "        condition = True\n",
        "        while condition:\n",
        "          for i,rbp in enumerate(rand_best_indices):\n",
        "            if rbp in true_pred_indices:\n",
        "              #rand_best_indices[i] = np.random.randint(K, size=1)\n",
        "              rand_best_indices[i] = rng.choice(K, size=1, replace=False)\n",
        "              condition = True\n",
        "              break\n",
        "            else:\n",
        "              condition = False\n",
        "        #print(true_pred_indices,rand_best_indices)\n",
        "        # set the coefficients at the right row \n",
        "        # that separates each class\n",
        "        if y[e] == 1:\n",
        "          X[e,true_pred_indices] = inv_weak_coefs # +  np.random.normal(0,sig_sigma)\n",
        "          X[e,rand_best_indices] = rand_strong_coefs + dist.sample(X[e,rand_best_indices].shape)  #np.random.normal(0,sig_sigma)\n",
        "\n",
        "        elif y[e] == 0:\n",
        "          X[e,true_pred_indices] = 0 #+  np.random.normal(0,sig_sigma) #-inv_weak_coefs \n",
        "          X[e,rand_best_indices] = 0 + dist.sample(X[e,rand_best_indices].shape)\n",
        "\n",
        "\n",
        "    #Z = np.c_[ X, y ]   \n",
        "    #print(np.corrcoef(Z.T))\n",
        "    print(X,y)\n",
        "    return y,X\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeCJncByyrU1"
      },
      "source": [
        "class ANDMaskState(optax.OptState):\n",
        "  \"\"\"Stateless.\"\"\" # Following optax code style\n",
        "\n",
        "def and_mask(agreement_threshold: float) -> optax.GradientTransformation:\n",
        "  def init_fn(_):\n",
        "    # Required by optax\n",
        "    return ANDMaskState()\n",
        "\n",
        "  def update_fn(updates, opt_state, params=None):\n",
        "\n",
        "    def and_mask(update):\n",
        "      # Compute the masked gradients for a single parameter tensor\n",
        "      mask = jnp.abs(jnp.mean(jnp.sign(update), 0)) >= agreement_threshold\n",
        "      mask = mask.astype(jnp.float32)\n",
        "      avg_update = jnp.mean(update, 0)\n",
        "      mask_t = mask.sum() / mask.size\n",
        "      update = mask * avg_update * (1. / (1e-10 + mask_t))\n",
        "      return update\n",
        "\n",
        "    del params # Following optax code style\n",
        "    \n",
        "    # Compute the masked gradients over all parameters\n",
        "\n",
        "    # jax.tree_map maps a function (lambda function in this case) over a pytree to produce a new pytree.\n",
        "    updates = jax.tree_map(lambda x: and_mask(x), updates)\n",
        "    return updates, opt_state\n",
        "\n",
        "  return transform.GradientTransformation(init_fn, update_fn)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dj9ugzYn_bek"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co_h9GFpx9C8"
      },
      "source": [
        "def sparse_logistic_regression_synthetic_data(train, test, adam_lr=1e-3, agreement_threshold=0.0,\n",
        "                               use_ilc=False, l1_coef=1e-4, l2_coef=1e-4,\n",
        "                               epochs=10001, Verbose=False, training=True, n_classes=2, normalizer=255.):\n",
        "\n",
        "  \n",
        "    OptState = Any\n",
        "    Batch = Mapping[str, np.ndarray]\n",
        "\n",
        "    def evaluate(params, validation_data, validation_labels):\n",
        "    \n",
        "        logits = net.apply(params, validation_data)\n",
        "        preds = jax.nn.log_softmax(logits)\n",
        "        pred_labels = np.argmax(preds, axis=1)\n",
        "\n",
        "        acc = accuracy(params, validation_data, validation_labels)\n",
        "        # As mentioned before, data is unbalanced, hence, the accuracy itself is not \n",
        "        # enough for evaluating the performance of the model.\n",
        "        # print(outputs,local_labels.cpu().detach().numpy())\n",
        "        cm = confusion_matrix(pred_labels.transpose(), validation_labels.transpose())\n",
        "        #sns.set_theme()\n",
        "        #plt.figure()\n",
        "        #ax = sns.heatmap(cm)\n",
        "        print('\\nConfusion Matrix: ', cm)\n",
        "        precision,recall,fscore,_ = precision_recall_fscore_support(validation_labels, pred_labels)\n",
        "        print('\\nAccuracy: ', acc,'\\nPrecision: ',precision,'\\nRecall: ', recall,'\\nF-score: ', fscore)\n",
        "\n",
        "\n",
        "    training_accs = []\n",
        "    testing_accs = []\n",
        "\n",
        "    def net_fn(batch) -> jnp.ndarray:\n",
        "    \n",
        "        x = jnp.array(batch, jnp.float32) / normalizer\n",
        "        mlp = hk.Sequential([\n",
        "            hk.Flatten(),\n",
        "            hk.Linear(n_classes),\n",
        "        ])\n",
        "        return mlp(x)\n",
        "\n",
        "\n",
        "    # Make the network and optimiser.\n",
        "    net = hk.without_apply_rng(hk.transform(net_fn))\n",
        "\n",
        "    \n",
        "       \n",
        "    # Training loss (cross-entropy).\n",
        "    def loss(params: hk.Params, batch, label) -> jnp.ndarray:\n",
        "        \"\"\"Compute the loss of the network, including L2.\"\"\"\n",
        "        logits = net.apply(params, batch)\n",
        "        labels = jax.nn.one_hot(label, n_classes)\n",
        "\n",
        "        # Cross Entropy Loss\n",
        "        softmax_xent = -jnp.sum(labels * jax.nn.log_softmax(logits))\n",
        "        softmax_xent /= labels.shape[0]\n",
        "        return softmax_xent\n",
        "        \n",
        "    # Regularization loss (L1,L2).\n",
        "    def regularization_loss(params: hk.Params) -> jnp.ndarray:\n",
        "        \"\"\"Compute the regularization loss of the network, applied after ILC.\"\"\"\n",
        "\n",
        "        # L1 Loss\n",
        "        sum_in_layer = lambda p: jnp.sum(jnp.abs(p))\n",
        "        sum_p_layers = [sum_in_layer(p) for p in jax.tree_leaves(params)]\n",
        "        l1_loss = sum(sum_p_layers)\n",
        "\n",
        "        # L2 Loss\n",
        "        l2_loss = 0.5 * sum(jnp.sum(jnp.square(p)) for p in jax.tree_leaves(params))\n",
        "\n",
        "        return l2_coef * l2_loss + l1_coef * l1_loss\n",
        "\n",
        "\n",
        "    @jax.jit\n",
        "    def gradient_per_sample(params, batch, label):\n",
        "        batch, label = jnp.expand_dims(batch,1), jnp.expand_dims(label,1)\n",
        "        return vmap(grad(loss), in_axes=(None, 0, 0))(params, batch, label)\n",
        "\n",
        "    \n",
        "    gradient = jax.jit(grad(loss))\n",
        "    gradient_reg = jax.jit(grad(regularization_loss))\n",
        "\n",
        "    # Evaluation metric (classification accuracy).\n",
        "    @jax.jit\n",
        "    def accuracy(params: hk.Params, batch, label) -> jnp.ndarray:\n",
        "        predictions = net.apply(params, batch)\n",
        "        return jnp.mean(jnp.argmax(predictions, axis=-1) == label)\n",
        "\n",
        "\n",
        "    \n",
        "    @jax.jit\n",
        "    def update(\n",
        "        params: hk.Params,\n",
        "        opt_state: OptState,\n",
        "        batch, label, agreement\n",
        "        ) -> Tuple[hk.Params, OptState]:\n",
        "        \"\"\"Learning rule (stochastic gradient descent).\"\"\"\n",
        "        # grads = jax.grad(loss)(params, batch, label)\n",
        "        # grads_masked = (gradient_per_sample if use_ilc else gradient)(params, batch, label) # (gradient_per_sample)(params, batch, label)\n",
        "        # sum_grad_masked_regularized = jax.tree_multimap(lambda x,y:x+y,grads_masked,gradient_reg(params))\n",
        "        # grads = sum_grad_masked_regularized\n",
        "        # updates, opt_state = opt.update(grads, opt_state)\n",
        "        # new_params = optax.apply_updates(params, updates)\n",
        "\n",
        "        grads_samples = gradient_per_sample(params, batch, label)\n",
        "        ANDmask = and_mask(agreement)\n",
        "\n",
        "        masked_grads,_ = ANDmask.update(grads_samples, opt_state)\n",
        "        reg_grads = gradient_reg(params)\n",
        "\n",
        "        sum_grad_masked_regularized = jax.tree_multimap(lambda x,y:x+y,masked_grads,reg_grads)\n",
        " \n",
        "        updates,_ = opt.update(sum_grad_masked_regularized, opt_state)\n",
        "\n",
        "        new_params = optax.apply_updates(params, updates)\n",
        "\n",
        "        return new_params, opt_state\n",
        "\n",
        "    # We maintain avg_params, the exponential moving average of the \"live\" params.\n",
        "    # avg_params is used only for evaluation.\n",
        "    # For more, see: https://doi.org/10.1137/0330046\n",
        "    @jax.jit\n",
        "    def ema_update(\n",
        "        avg_params: hk.Params,\n",
        "        new_params: hk.Params,\n",
        "        epsilon: float = 0.001,\n",
        "    ) -> hk.Params:\n",
        "        return jax.tree_multimap(lambda p1, p2: (1 - epsilon) * p1 + epsilon * p2,\n",
        "                                avg_params, new_params)\n",
        "\n",
        "    if training is False:\n",
        "        return net, accuracy\n",
        "    else:\n",
        "        if use_ilc:\n",
        "\n",
        "            use_ilc = False\n",
        "\n",
        "            # opt = optax.chain(and_mask(agreement_threshold) if use_ilc else optax.identity(),optax.adam(adam_lr))\n",
        "\n",
        "            # schedule_fn = optax.exponential_decay(adam_lr, # Note the minus sign!\n",
        "            # 1,\n",
        "            # 0.9)\n",
        "            # opt = optax.chain(optax.sgd(adam_lr), optax.scale_by_schedule(schedule_fn)) # Or Adam could be used\n",
        "            opt = optax.chain(optax.sgd(adam_lr)) # Or Adam could be used\n",
        "\n",
        "            # Initialize network and optimiser; note we draw an input to get shapes.\n",
        "            params = avg_params = net.init(jax.random.PRNGKey(42), next(train)[0])\n",
        "            opt_state = opt.init(params)\n",
        "\n",
        "            # Train/eval loop. WITHOUT ILC\n",
        "            print(\"Begin training with ILC\")\n",
        "            for step in range(np.int(.5*epochs)):\n",
        "                if step % np.int(epochs/10) == 0:\n",
        "                    # Periodically evaluate classification accuracy on train & test sets.\n",
        "                    Batch = next(train)\n",
        "                    train_accuracy = accuracy(avg_params, Batch[0], Batch[1])\n",
        "                    train_accuracy = jax.device_get(train_accuracy)\n",
        "                    Batch = next(test)\n",
        "                    test_accuracy = accuracy(avg_params, Batch[0], Batch[1])\n",
        "                    test_accuracy = jax.device_get(test_accuracy)\n",
        "                    training_accs.append(train_accuracy)\n",
        "                    testing_accs.append(test_accuracy)\n",
        "                    if Verbose:\n",
        "                        print(f\"[ILC Off, Step {step}] Train accuracy/Test accuracy: \"\n",
        "                                f\"{train_accuracy:.3f} / {test_accuracy:.3f}.\")\n",
        "\n",
        "                # Do SGD on a batch of training examples.\n",
        "                Batch = next(train)\n",
        "                params, opt_state = update(params, opt_state, Batch[0], Batch[1], 0.)\n",
        "                avg_params = ema_update(avg_params, params)\n",
        "\n",
        "\n",
        "            use_ilc = True\n",
        "\n",
        "            # opt = optax.chain(optax.adam(adam_lr))\n",
        "\n",
        "            # Initialize network and optimiser; note we draw an input to get shapes.\n",
        "            opt_state = opt.init(params)\n",
        "            \n",
        "            # Train/eval loop. WITH ILC\n",
        "            for step in range(np.int(.5*epochs)):\n",
        "                if step % np.int(epochs/10) == 0:\n",
        "                    # Periodically evaluate classification accuracy on train & test sets.\n",
        "                    Batch = next(train)\n",
        "                    train_accuracy = accuracy(avg_params, Batch[0], Batch[1])\n",
        "                    train_accuracy = jax.device_get(train_accuracy)\n",
        "                    Batch = next(test)\n",
        "                    test_accuracy = accuracy(avg_params, Batch[0], Batch[1])\n",
        "                    test_accuracy = jax.device_get(test_accuracy)\n",
        "                    training_accs.append(train_accuracy)\n",
        "                    testing_accs.append(test_accuracy)\n",
        "                    if Verbose:\n",
        "                        print(f\"[ILC On, Step {step}] Train accuracy/Test accuracy: \"\n",
        "                                f\"{train_accuracy:.3f} / {test_accuracy:.3f}.\")\n",
        "\n",
        "                # Do SGD on a batch of training examples.\n",
        "                Batch = next(train)\n",
        "                params, opt_state = update(params, opt_state, Batch[0], Batch[1], agreement_threshold)\n",
        "                avg_params = ema_update(avg_params, params)\n",
        "\n",
        "            print(\"Evaluating with ILC:\")\n",
        "            Batch = next(test)\n",
        "            test_data, test_labels = Batch[0], Batch[1]\n",
        "            evaluate(avg_params, test_data, test_labels)\n",
        "\n",
        "\n",
        "            return params, training_accs, testing_accs\n",
        "\n",
        "        else:\n",
        "\n",
        "             # schedule_fn = optax.exponential_decay(adam_lr, # Note the minus sign!\n",
        "            # 1,\n",
        "            # 0.9)\n",
        "            # opt = optax.chain(optax.sgd(adam_lr), optax.scale_by_schedule(schedule_fn)) # Or Adam could be used\n",
        "            opt = optax.chain(optax.sgd(adam_lr))\n",
        "\n",
        "            # Initialize network and optimiser; note we draw an input to get shapes.\n",
        "            params = avg_params = net.init(jax.random.PRNGKey(42), next(train)[0])\n",
        "            opt_state = opt.init(params)\n",
        "\n",
        "            use_ilc=False\n",
        "            # Train/eval loop. \n",
        "            print(\"Begin training without ILC\")\n",
        "            for step in range(np.int(epochs)):\n",
        "                if step % np.int(epochs/10) == 0:\n",
        "                    # Periodically evaluate classification accuracy on train & test sets.\n",
        "                    Batch = next(train)\n",
        "                    train_accuracy = accuracy(avg_params, Batch[0], Batch[1])\n",
        "                    train_accuracy = jax.device_get(train_accuracy)\n",
        "                    Batch = next(test)\n",
        "                    test_accuracy = accuracy(avg_params, Batch[0], Batch[1])\n",
        "                    test_accuracy = jax.device_get(test_accuracy)\n",
        "                    training_accs.append(train_accuracy)\n",
        "                    testing_accs.append(test_accuracy)\n",
        "                    if Verbose:\n",
        "                        print(f\"[ADAM, Step {step}] Train accuracy/Test accuracy: \"\n",
        "                                f\"{train_accuracy:.3f} / {test_accuracy:.3f}.\")\n",
        "                        \n",
        "                # Do SGD on a batch of training examples.\n",
        "                Batch = next(train)\n",
        "                params, opt_state = update(params, opt_state, Batch[0], Batch[1], 0.)\n",
        "                avg_params = ema_update(avg_params, params)\n",
        "            \n",
        "            print(\"Evaluating without ILC:\")\n",
        "            Batch = next(test)\n",
        "            test_data, test_labels = Batch[0], Batch[1]\n",
        "            evaluate(avg_params, test_data, test_labels)\n",
        "\n",
        "            \n",
        "            return params, training_accs, testing_accs\n"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tJ7TWnp_W3F"
      },
      "source": [
        "### Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7bmCtmN_WHf"
      },
      "source": [
        "OptState = Any\r\n",
        "Batch = Mapping[str, np.ndarray]\r\n",
        "\r\n",
        "\r\n",
        "def linear_regression(train=None, test=None, adam_lr=0.3, agreement_threshold=0.0,\r\n",
        "                               use_ilc=False, l1_coef=1e-4, l2_coef=1e-4,\r\n",
        "                               epochs=1001, Verbose=False, training=True, normalizer=255.):\r\n",
        "  \r\n",
        "\r\n",
        "\r\n",
        "    training_loss = []\r\n",
        "    testing_loss = []\r\n",
        "\r\n",
        "    def net_fn(batch) -> jnp.ndarray:\r\n",
        "        x = jnp.array(batch, jnp.float32) / normalizer\r\n",
        "        mlp = hk.Sequential([\r\n",
        "            hk.Flatten(),\r\n",
        "            hk.Linear(1, with_bias=False)\r\n",
        "        ])\r\n",
        "        return mlp(x)\r\n",
        "\r\n",
        "    \r\n",
        "\r\n",
        "    # Make the network and optimiser.\r\n",
        "    net = hk.without_apply_rng(hk.transform(net_fn))\r\n",
        "        \r\n",
        "    # Training loss (cross-entropy).\r\n",
        "    def loss(params: hk.Params, batch, label) -> jnp.ndarray:\r\n",
        "        \"\"\"Compute the loss of the network, including L2.\"\"\"\r\n",
        "        logits = net.apply(params, batch)\r\n",
        "        \r\n",
        "        msl = 0.5 * jnp.sum(jnp.power(logits - label,2)) / batch.shape[0]\r\n",
        "\r\n",
        "        return msl \r\n",
        "\r\n",
        "        \r\n",
        "    # Regularization loss (L1,L2).\r\n",
        "    def regularization_loss(params: hk.Params) -> jnp.ndarray:\r\n",
        "        \"\"\"Compute the regularization loss of the network, applied after ILC.\"\"\"\r\n",
        "\r\n",
        "        # L1 Loss\r\n",
        "        sum_in_layer = lambda p: jnp.sum(jnp.abs(p))\r\n",
        "        sum_p_layers = [sum_in_layer(p) for p in jax.tree_leaves(params)]\r\n",
        "        l1_loss = sum(sum_p_layers)\r\n",
        "\r\n",
        "        # L2 Loss\r\n",
        "        l2_loss = 0.5 * sum(jnp.sum(jnp.square(p)) for p in jax.tree_leaves(params))\r\n",
        "\r\n",
        "        return l2_coef * l2_loss + l1_coef * l1_loss\r\n",
        "\r\n",
        "    @jax.jit\r\n",
        "    def gradient_per_sample(params, batch, label):\r\n",
        "        batch, label = jnp.expand_dims(batch,1), jnp.expand_dims(label,1)\r\n",
        "        return vmap(grad(loss), in_axes=(None, 0, 0))(params, batch, label)\r\n",
        "\r\n",
        "    gradient = jax.jit(grad(loss))\r\n",
        "    gradient_reg = jax.jit(grad(regularization_loss))\r\n",
        "\r\n",
        "    @jax.jit\r\n",
        "    def update(\r\n",
        "        params: hk.Params,\r\n",
        "        opt_state: OptState,\r\n",
        "        batch, label, agreement\r\n",
        "        ) -> Tuple[hk.Params, OptState]:\r\n",
        "        \"\"\"Learning rule (stochastic gradient descent).\"\"\"\r\n",
        "        # grads_masked = (gradient_per_sample if use_ilc else gradient)(params, batch, label) # (gradient_per_sample)(params, batch, label)\r\n",
        "        # sum_grad_masked_regularized = jax.tree_multimap(lambda x,y:x+y,grads_masked,gradient_reg(params))\r\n",
        "        # grads = sum_grad_masked_regularized\r\n",
        "        # updates, opt_state = opt.update(grads, opt_state)\r\n",
        "        # new_params = optax.apply_updates(params, updates)\r\n",
        "\r\n",
        "        # grads = gradient(params, batch, label)\r\n",
        "        grads_samples = gradient_per_sample(params, batch, label)\r\n",
        "        ANDmask = and_mask(agreement)\r\n",
        "\r\n",
        "        masked_grads,_ = ANDmask.update(grads_samples, opt_state)\r\n",
        "        reg_grads = gradient_reg(params)\r\n",
        "\r\n",
        "        sum_grad_masked_regularized = jax.tree_multimap(lambda x,y:x+y,masked_grads,reg_grads)\r\n",
        " \r\n",
        "        updates,_ = opt.update(sum_grad_masked_regularized, opt_state)\r\n",
        "\r\n",
        "        new_params = optax.apply_updates(params, updates)\r\n",
        "\r\n",
        "        return new_params, opt_state\r\n",
        "        \r\n",
        "    # We maintain avg_params, the exponential moving average of the \"live\" params.\r\n",
        "    # avg_params is used only for evaluation.\r\n",
        "    # For more, see: https://doi.org/10.1137/0330046\r\n",
        "    @jax.jit\r\n",
        "    def ema_update(\r\n",
        "        avg_params: hk.Params,\r\n",
        "        new_params: hk.Params,\r\n",
        "        epsilon: float = 0.01,\r\n",
        "    ) -> hk.Params:\r\n",
        "        return jax.tree_multimap(lambda p1, p2: (1 - epsilon) * p1 + epsilon * p2,\r\n",
        "                                avg_params, new_params)\r\n",
        "    \r\n",
        "    if training is False:\r\n",
        "        return net\r\n",
        "    else:\r\n",
        "        if(use_ilc):\r\n",
        "\r\n",
        "            use_ilc = False\r\n",
        "\r\n",
        "            # opt = optax.chain(and_mask(agreement_threshold) if use_ilc else optax.identity(),optax.adam(adam_lr))\r\n",
        "\r\n",
        "            # schedule_fn = optax.exponential_decay(adam_lr, # Note the minus sign!\r\n",
        "            # 1,\r\n",
        "            # 0.9)\r\n",
        "            # opt = optax.chain(optax.sgd(adam_lr), optax.scale_by_schedule(schedule_fn)) # Or Adam could be used\r\n",
        "            opt = optax.chain(optax.adam(adam_lr)) # Or Adam could be used\r\n",
        "\r\n",
        "            # Initialize network and optimiser; note we draw an input to get shapes.\r\n",
        "            params = avg_params = net.init(jax.random.PRNGKey(42), next(train)[0])\r\n",
        "            opt_state = opt.init(params)\r\n",
        "\r\n",
        "            # Train/eval loop. WITHOUT ILC\r\n",
        "            print(\"Begin training with ILC\")\r\n",
        "            for step in range(np.int(.5*epochs)):\r\n",
        "                if step % np.int(epochs/10) == 0:\r\n",
        "                    # Periodically evaluate classification accuracy on train & test sets.\r\n",
        "                    Batch = next(train)\r\n",
        "                    train_loss = loss(avg_params, Batch[0], Batch[1])\r\n",
        "                    train_loss = jax.device_get(train_loss)\r\n",
        "                    Batch = next(test)\r\n",
        "                    test_loss = loss(avg_params, Batch[0], Batch[1])\r\n",
        "                    test_loss = jax.device_get(test_loss)\r\n",
        "                    training_loss.append(train_accuracy)\r\n",
        "                    testing_loss.append(test_accuracy)\r\n",
        "                    if Verbose:\r\n",
        "                        print(f\"[ILC Off, Step {step}] Train loss/Test loss: \"\r\n",
        "                                f\"{train_loss:.3f} / {test_loss:.3f}.\")\r\n",
        "\r\n",
        "                # Do SGD on a batch of training examples.\r\n",
        "                Batch = next(train)\r\n",
        "                params, opt_state = update(params, opt_state, Batch[0], Batch[1], 0.)\r\n",
        "                avg_params = ema_update(avg_params, params)\r\n",
        "            \r\n",
        "\r\n",
        "            use_ilc = True\r\n",
        "\r\n",
        "            \r\n",
        "            # Train/eval loop. WITH ILC\r\n",
        "            for step in range(np.int(.5*epochs)):\r\n",
        "                if step % np.int(epochs/10) == 0:\r\n",
        "                    # Periodically evaluate classification accuracy on train & test sets.\r\n",
        "                    Batch = next(train)\r\n",
        "                    train_loss = loss(avg_params, Batch[0], Batch[1])\r\n",
        "                    train_loss = jax.device_get(train_loss)\r\n",
        "                    Batch = next(test)\r\n",
        "                    test_loss = loss(avg_params, Batch[0], Batch[1])\r\n",
        "                    test_loss = jax.device_get(test_loss)\r\n",
        "                    training_loss.append(train_accuracy)\r\n",
        "                    testing_loss.append(test_accuracy)\r\n",
        "                    if Verbose:\r\n",
        "                        print(f\"[ILC On, Step {step}] Train loss/Test loss: \"\r\n",
        "                                f\"{train_loss:.3f} / {test_loss:.3f}.\")\r\n",
        "\r\n",
        "                # Do SGD on a batch of training examples.\r\n",
        "                Batch = next(train)\r\n",
        "                params, opt_state = update(params, opt_state, Batch[0], Batch[1], agreement_threshold)\r\n",
        "                avg_params = ema_update(avg_params, params)\r\n",
        "          \r\n",
        "\r\n",
        "            return params, training_loss, testing_loss\r\n",
        "\r\n",
        "        else:\r\n",
        "                \r\n",
        "            # schedule_fn = optax.exponential_decay(adam_lr, # Note the minus sign!\r\n",
        "            # 1,\r\n",
        "            # 0.9)\r\n",
        "            # opt = optax.chain(optax.sgd(adam_lr), optax.scale_by_schedule(schedule_fn)) # Or Adam could be used\r\n",
        "            opt = optax.chain(optax.adam(adam_lr))\r\n",
        "\r\n",
        "            # Initialize network and optimiser; note we draw an input to get shapes.\r\n",
        "            params = avg_params = net.init(jax.random.PRNGKey(42), dataset_train['x'][0])\r\n",
        "            opt_state = opt.init(params)\r\n",
        "\r\n",
        "            # Train/eval loop. \r\n",
        "            print(\"Begin training without ILC\")\r\n",
        "            for step in range(np.int(epochs)):\r\n",
        "                if step % np.int(epochs/10) == 0:\r\n",
        "                    # Periodically evaluate classification accuracy on train & test sets.\r\n",
        "                    Batch = next(train)\r\n",
        "                    train_loss = loss(avg_params, Batch[0], Batch[1])\r\n",
        "                    train_loss = jax.device_get(train_loss)\r\n",
        "                    Batch = next(test)\r\n",
        "                    test_loss = loss(avg_params, Batch[0], Batch[1])\r\n",
        "                    test_loss = jax.device_get(test_loss)\r\n",
        "                    training_loss.append(train_accuracy)\r\n",
        "                    testing_loss.append(test_accuracy)\r\n",
        "                    if Verbose:\r\n",
        "                        print(f\"[ADAM, Step {step}] Train loss/Test loss: \"\r\n",
        "                                f\"{train_loss:.3f} / {test_loss:.3f}.\")\r\n",
        "                        \r\n",
        "                # Do SGD on a batch of training examples.\r\n",
        "                Batch = next(train)\r\n",
        "                params, opt_state = update(params, opt_state, Batch[0], Batch[1], 0.)\r\n",
        "                avg_params = ema_update(avg_params, params)\r\n",
        "\r\n",
        "            \r\n",
        "            return params, training_loss, testing_loss\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hm9UUFbBx9C4"
      },
      "source": [
        "rng = default_rng()\n",
        "E = 5000 # number of environments\n",
        "train_batch_size = np.float32(.5*E)\n",
        "sigma = [6] # amount of noise \n",
        "signal_noise = [4]\n",
        "K = 45 # number of predictors (at least 40)\n",
        "n_informative = 20\n",
        "n_redundant = 0\n",
        "nepochs = 1000\n",
        "weights=[0.7,0.3]\n",
        "rand_strong_coefs = 5.\n",
        "inv_weak_coefs = 0.1\n",
        "use_sklearn = False\n",
        "# true invariant predictor indices \n",
        "#true_pred_indices = np.random.randint(K, size=n_informative)\n",
        "true_pred_indices = rng.choice(K, size=n_informative, replace=False)\n",
        "true_pred_indices\n",
        "agreement = 0.4\n",
        "l1_sgd = 0.\n",
        "l2_sgd = 0.\n",
        "l1_ilc = 1e-4\n",
        "l2_ilc = 1e-4"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JUMVsA1x9C_",
        "outputId": "cfc92767-5c4b-42d5-ef76-3cf4cb8bdce1"
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "\n",
        "for global_noise in sigma:\n",
        "    for sig_noise in signal_noise:\n",
        "        print(f\"Global noise: {global_noise}, Signal noise: {sig_noise}\")\n",
        "        y,x = generate_synthetic_dataset(E,train_batch_size,global_noise,sig_noise,K,rand_strong_coefs,inv_weak_coefs,true_pred_indices,n_informative,n_redundant,use_sklearn)\n",
        "        \n",
        "        test_batch_size = E-train_batch_size\n",
        "        # ds = tfds.load(\"mnist:3.*.*\", split=splits[m]).cache().repeat()\n",
        "        # print(ds)\n",
        "        all_dataset = tf.data.Dataset.from_tensor_slices((x, y)).repeat()\n",
        "        train_dataset = all_dataset.take(train_batch_size).repeat()\n",
        "        test_dataset = all_dataset.skip(train_batch_size).repeat()\n",
        "        # print(train_dataset)\n",
        "        # print(test_dataset)\n",
        "        train_dataset = train_dataset.shuffle(10.0 * train_batch_size, seed=0)\n",
        "        train_dataset = train_dataset.batch(train_batch_size)\n",
        "        train_dataset = iter(tfds.as_numpy(train_dataset))\n",
        "\n",
        "        test_dataset = test_dataset.shuffle(10.0 * test_batch_size, seed=0)\n",
        "        test_dataset = test_dataset.batch(test_batch_size)\n",
        "        test_dataset = iter(tfds.as_numpy(test_dataset))\n",
        "        \n",
        "        print(\"***Using Elastic Net***\")\n",
        "        # sgd\n",
        "        params = sparse_logistic_regression_synthetic_data(train_dataset, test_dataset, adam_lr=1e-1, agreement_threshold=0.,\n",
        "                                       use_ilc=False, l1_coef=l1_sgd, l2_coef=l2_sgd,\n",
        "                                       epochs=nepochs, Verbose=True, training=True, n_classes=2, normalizer=1.)\n",
        "        # print(params)\n",
        "        # sgd-ilc\n",
        "        params = sparse_logistic_regression_synthetic_data(train_dataset, test_dataset, adam_lr=1e-1, agreement_threshold=agreement,\n",
        "                                use_ilc=True, l1_coef=l1_ilc, l2_coef=l2_ilc,\n",
        "                                epochs=nepochs, Verbose=True, training=True, n_classes=2, normalizer=1.)\n",
        "        # print(params)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Global noise: 6, Signal noise: 4\n",
            "[[ 7.35 -5.93  0.1  ...  3.05 10.92  0.1 ]\n",
            " [ 0.39 -0.96  0.1  ... 12.49  0.33  0.1 ]\n",
            " [ 0.67 -3.03  0.   ... -7.7  -2.56  0.  ]\n",
            " ...\n",
            " [ 5.74  1.91  0.   ... -8.06 -6.19  0.  ]\n",
            " [-4.65 12.38  0.   ... -3.52  6.35  0.  ]\n",
            " [ 2.47 -6.96  0.1  ...  7.25  4.53  0.1 ]] [1 1 0 ... 0 0 1]\n",
            "***Using Elastic Net***\n",
            "Begin training without ILC\n",
            "[ADAM, Step 0] Train accuracy/Test accuracy: 0.494 / 0.496.\n",
            "[ADAM, Step 100] Train accuracy/Test accuracy: 0.488 / 0.500.\n",
            "[ADAM, Step 200] Train accuracy/Test accuracy: 0.491 / 0.508.\n",
            "[ADAM, Step 300] Train accuracy/Test accuracy: 0.518 / 0.504.\n",
            "[ADAM, Step 400] Train accuracy/Test accuracy: 0.516 / 0.527.\n",
            "[ADAM, Step 500] Train accuracy/Test accuracy: 0.534 / 0.527.\n",
            "[ADAM, Step 600] Train accuracy/Test accuracy: 0.534 / 0.550.\n",
            "[ADAM, Step 700] Train accuracy/Test accuracy: 0.554 / 0.559.\n",
            "[ADAM, Step 800] Train accuracy/Test accuracy: 0.570 / 0.589.\n",
            "[ADAM, Step 900] Train accuracy/Test accuracy: 0.623 / 0.633.\n",
            "Evaluating without ILC:\n",
            "\n",
            "Confusion Matrix:  [[819 459]\n",
            " [391 831]]\n",
            "\n",
            "Accuracy:  0.65999997 \n",
            "Precision:  [0.64 0.68] \n",
            "Recall:  [0.68 0.64] \n",
            "F-score:  [0.66 0.66]\n",
            "Begin training with ILC\n",
            "[ILC Off, Step 0] Train accuracy/Test accuracy: 0.492 / 0.491.\n",
            "[ILC Off, Step 100] Train accuracy/Test accuracy: 0.478 / 0.483.\n",
            "[ILC Off, Step 200] Train accuracy/Test accuracy: 0.508 / 0.502.\n",
            "[ILC Off, Step 300] Train accuracy/Test accuracy: 0.508 / 0.515.\n",
            "[ILC Off, Step 400] Train accuracy/Test accuracy: 0.521 / 0.508.\n",
            "[ILC On, Step 0] Train accuracy/Test accuracy: 0.530 / 0.525.\n",
            "[ILC On, Step 100] Train accuracy/Test accuracy: 0.538 / 0.552.\n",
            "[ILC On, Step 200] Train accuracy/Test accuracy: 0.570 / 0.552.\n",
            "[ILC On, Step 300] Train accuracy/Test accuracy: 0.601 / 0.614.\n",
            "[ILC On, Step 400] Train accuracy/Test accuracy: 0.623 / 0.632.\n",
            "Evaluating with ILC:\n",
            "\n",
            "Confusion Matrix:  [[746 409]\n",
            " [478 867]]\n",
            "\n",
            "Accuracy:  0.64519995 \n",
            "Precision:  [0.65 0.64] \n",
            "Recall:  [0.61 0.68] \n",
            "F-score:  [0.63 0.66]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQ5GBWwp2TRF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}